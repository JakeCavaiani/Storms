## DoD 2020 - compile and do initial cleaning of SUNA files from internal hard drives ##
# press Command+Option+O to collapse all sections and get an overview of the workflow! #

#### Read me ####
# The purpose of this script is to process and plot SUNA data that has been downloaded directly off the SUNAs' internal hard drives. This data is formatted differently than telemetry data and so must be processed seperately!

# >>>EDIT HERE<<< in section header means some code in that section needs to be manually edited depending on circumstances. Most often, this is IDing or commenting in/out code for sites that you are or aren't processing data for. 
## TIP! Comment out a section of code by highlighting it and press Command+Shift+C (on macs). Comment it back in the same way. 

# there is one line of code in the "Add column headers and format date/time" that must be edited if this script is adapted for data from other calendar years (i.e., not 2020). It is identified with the comment: "###>>> NOTE: MUST CHANGE THIS EVERY YEAR!!!! <<<###"
## For 2020 data, this is: origin <- as.POSIXct("2019-12-31 00:00:00", tz="GMT")
## For 2021 data, you'd edit this to say origin <- as.POSIXct("2020-12-31 00:00:00", tz="GMT")


#### Load libraries ####
library(tidyverse)
library(lubridate)
library(data.table)
library(rio)
library(ggplot2)
library(scales)
library(psych)
library(googledrive)
library(purrr)
library(here)

#### Load field dates and times ####
#These are stored in field_datetime.csv on Drive (DoD project/2020 AK sensors)

dir.create(here("SUNA_data"))
dir.create(here("SUNA_data", "from_internal_harddrive"))
dir.create(here("SUNA_data", "from_internal_harddrive", "raw"))
dir.create(here("SUNA_data", "from_internal_harddrive", "plots"))
dir.create(here("SUNA_data", "from_internal_harddrive", "processed"))

setwd(here("SUNA_data", "from_internal_harddrive", "raw"))

fieldurl <- "https://drive.google.com/drive/folders/14snoMg3FuhLl0SJFh8iPDVgnaP-3ozN1"
fieldtimes <- drive_get(as_id(fieldurl))
field_glist <- drive_ls(fieldtimes, pattern = "Field date_time")
walk(field_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))
field_datetime <- read.csv("Field date_time.csv")

field_datetime$DATE = as.Date(as.character(field_datetime$DATE), "%y%m%d")
# format time cols
field_datetime$TIME.Last.good.sonde.measurements = as.POSIXct(paste(field_datetime$DATE, field_datetime$TIME.Last.good.sonde.measurements), "%Y-%m-%d %H:%M", tz="America/Anchorage")
#
field_datetime$TIME.First.returned.good.sonde.measurements = as.POSIXct(paste(field_datetime$DATE, field_datetime$TIME.First.returned.good.sonde.measurements), "%Y-%m-%d %H:%M", tz="America/Anchorage")
#
field_datetime$TIME.Water.samples = as.POSIXct(paste(field_datetime$DATE, field_datetime$TIME.Water.samples), "%Y-%m-%d %H:%M", tz="America/Anchorage")
#
field_datetime$TIME.Q.Salt.Slug = as.POSIXct(paste(field_datetime$DATE, field_datetime$TIME.Q.Salt.Slug), "%Y-%m-%d %H:%M", tz="America/Anchorage")
#
field_datetime$TIME.Q.Flowmeter = as.POSIXct(paste(field_datetime$DATE, field_datetime$TIME.Q.Flowmeter), "%Y-%m-%d %H:%M", tz="America/Anchorage")
#
field_datetime$TIME.Other = as.POSIXct(paste(field_datetime$DATE, field_datetime$TIME.Other), "%Y-%m-%d %H:%M", tz="America/Anchorage")

#### Download raw SUNA data from Drive ####
# Make sub-directories where data will be stored
dir.create(file.path("FRCH"))
dir.create(file.path("MOOS"))
dir.create(file.path("POKE"))
dir.create(file.path("STRT"))
dir.create(file.path("VAUL"))

## Download data from Google Drive
FRCHurl <- "https://drive.google.com/drive/folders/1X5O_H71HnUx09Gt9WZRTJ23CMbO-TM0p"
MOOSurl <- "https://drive.google.com/drive/folders/1Fxn13LCZMTi8fVaTRpTltdqrfiNrFvWx"
POKEurl <- "https://drive.google.com/drive/folders/1FBhhUB1i8AduMJTCaRtZ9k7De5w8E_c-"
STRTurl <- "https://drive.google.com/drive/folders/1EYq0xzj0zEXDUy5qNKKEjso_mBr7OzQo"
VAULurl <- "https://drive.google.com/drive/folders/1F7E8HGeDtFhsLyoGZFPTol_HEzN7nIXx"

# First time this will ask to cache OAuth credentials. Open browser. Run the drive_get commands below. Enter "1" to allow cache. Go to browser, log in to Google, and allow API access. 
FRCH_new <- drive_get(as_id(FRCHurl))
MOOS_new <- drive_get(as_id(MOOSurl))
POKE_new <- drive_get(as_id(POKEurl))
STRT_new <- drive_get(as_id(STRTurl))
VAUL_new <- drive_get(as_id(VAULurl))

FRCH_glist <- drive_ls(FRCH_new, type = "csv")
MOOS_glist <- drive_ls(MOOS_new, type = "csv")
POKE_glist <- drive_ls(POKE_new, type = "csv")
STRT_glist <- drive_ls(STRT_new, type = "csv")
VAUL_glist <- drive_ls(VAUL_new, type = "csv")

#I know this is hinky with the switching of working directory. The drive_download command does not currently allow downloading into subdirectories.
setwd(here("SUNA_data", "from_internal_harddrive", "raw", "FRCH"))
walk(FRCH_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))

setwd(here("SUNA_data", "from_internal_harddrive", "raw", "MOOS"))
walk(MOOS_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))

setwd(here("SUNA_data", "from_internal_harddrive", "raw", "POKE"))
walk(POKE_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))

setwd(here("SUNA_data", "from_internal_harddrive", "raw", "STRT"))
walk(STRT_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))

setwd(here("SUNA_data", "from_internal_harddrive", "raw", "VAUL"))
walk(VAUL_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))

setwd(here("SUNA_data", "from_internal_harddrive", "raw"))
# Should be back to raw directory

### Stitch together manually downloaded SUNA data ###
FRCHfile_list <- list.files(path = "./FRCH/", 
                            recursive=F, 
                            pattern=".CSV", 
                            full.names=TRUE)

SUNA.FRCH<-do.call("rbind", lapply(FRCHfile_list, 
                                   read.csv, 
                                   stringsAsFactors=FALSE, 
                                   skip=14, header=FALSE))

STRTfile_list <- list.files(path = "./STRT/", 
                            recursive=F, 
                            pattern=".CSV", 
                            full.names=TRUE)

SUNA.STRT<-do.call("rbind", lapply(STRTfile_list, 
                                   read.csv, 
                                   stringsAsFactors=FALSE, 
                                   skip=14, header=FALSE))

MOOSfile_list <- list.files(path = "./MOOS/",
                            recursive=F,
                            pattern=".CSV",
                            full.names=TRUE)

SUNA.MOOS<-do.call("rbind", lapply(MOOSfile_list,
                                   read.csv,
                                   stringsAsFactors=FALSE,
                                   skip=14, header=FALSE))

POKEfile_list <- list.files(path = "./POKE/",
                            recursive=F,
                            pattern=".CSV",
                            full.names=TRUE)

SUNA.POKE<-do.call("rbind", lapply(POKEfile_list,
                                   read.csv,
                                   stringsAsFactors=FALSE,
                                   skip=14, header=FALSE))

VAULfile_list <- list.files(path="./VAUL/",
                            recursive=F,
                            pattern=".CSV",
                            full.names=TRUE)

SUNA.VAUL<-do.call("rbind", lapply(VAULfile_list,
                                   read.csv,
                                   stringsAsFactors=FALSE,
                                   skip=14, header=FALSE))

#Set working directory back to DoD_2021
setwd(here())

#### ID sites you are processing data for >>>EDIT HERE<<< ####

sitedataz = list(SUNA.FRCH, SUNA.MOOS, SUNA.POKE, SUNA.STRT, SUNA.VAUL)
#sitedataz = list(SUNA.POKE)

# IMPORTANT: name sites below in the same order as they are listed above! Note that they are read into the list as SUNA.SITECODE (no quotes) above and just "SITECODE" (with quotes) below
names(sitedataz) = c("FRCH", "MOOS", "POKE", "STRT", "VAUL")
#names(sitedataz) = c("POKE")
sitenamez = names(sitedataz)
#

#### Add column headers and format date/time ####

pre<-"ch"
suff<-seq(12:267)
ch<-paste(pre, suff)
SUNAnames<-c("ID", "date_yearday", "time_fhoursUTC", "nitrateuM", "nitratemgL", "abs254", 
             "abs350", "brtrace", "specave", "darkvaluefit", "inttimefac", ch, "int_TC", 
             "spec_TC", "lamp_TC", "lamptimecum", "relhum", "mainV", "lampV", "intV", 
             "mainmA", "fit1", "fit2", "fitbase1", "fitbase2", "fitRMSE", "CTDtime", 
             "CTDsal", "CTDT", "CTDdBar", "checksum")

for (i in 1:length(sitedataz)){
  names(sitedataz[[i]])<-SUNAnames
}

for (i in sitenamez){
  ## add time stamps in AK tz
  # create separate year and day columns
  year_day <- t(sapply(sitedataz[[i]]$date_yearday, function(x) substring(x, first=c(1,5), last=c(4,7))))
  year_day<-as.data.frame(year_day)
  names(year_day)<-c("year", "day")
  #year_day$day<-as.numeric(levels(year_day$day))[year_day$day]
  #year_day$year<-as.numeric(levels(year_day$year))[year_day$year]
  year_day_num=matrix(0, nrow=  nrow(year_day), ncol=2)
  for (y in 1:nrow(year_day)){
    if(class(year_day$day)=="character"){ 
      year_day_num[y,2] = as.numeric(year_day[y,2])
    }
    else
      year_day_num[y,2] = as.numeric(levels(year_day[y,2]))[year_day[y,2]]
    
    if(class(year_day$year)=="character"){ 
      year_day_num[y,1] = as.numeric(year_day[y,1])
    }
    else
      year_day_num[y,1] = as.numeric(levels(year_day[y,1]))[year_day[y,1]] 
  }
  year_day<-as.data.frame(year_day_num)
  names(year_day)<-c("year", "day")
  sitedataz[[i]]<-cbind(sitedataz[[i]], year_day)
  # combine hours and julian day into fractional days
  sitedataz[[i]]$day_timeUTC<-sitedataz[[i]]$day+(sitedataz[[i]]$time_fhoursUTC/24)
  # assign year to 2020 data ###>>> NOTE: MUST CHANGE THIS EVERY YEAR!!!! <<<###
  origin <- as.POSIXct("2019-12-31 00:00:00", tz="GMT")
  sitedataz[[i]]$date_timeUTC<-origin + sitedataz[[i]]$day_timeUTC * 3600 * 24
  # convert from UTC to AKDT
  sitedataz[[i]]$datetimeAK<-as.POSIXct(format(sitedataz[[i]]$date_timeUTC, tz="America/Anchorage", usetz=TRUE))
  tz(sitedataz[[i]]$datetimeAK) = "America/Anchorage"
  sitedataz[[i]]$date = date(sitedataz[[i]]$datetimeAK)
  sitedataz[[i]]$notes=NA
}

#### Remove all data from site setup & replace chem data from sonde servicing with NAs ####
##########*************** TKH working here
# remove all data rows from site setup:
# setuptimez = list()
# for(i in sitenamez){
#   setuptimez[[i]] = field_datetime$TIME.First.returned.good.sonde# .measurements[field_datetime$SITE.CODE==i & field_datetime$Notes=="setup"]
#   sitedataz[[i]] = sitedataz[[i]][sitedataz[[i]]$datetimeAK > setuptimez[[i]],]
}

# replace data during sonde servicing times with NA:
# # remove row describing a site visit to Vault on 200706 when the cage was already out of the water. This visit has no "first good measurement" and we do not want to remove data preceding it until we've looked at the data. Could instead replace the NA in "first good" column with the time it appears to have left the water to remove the in-air data.   
# field_datetime = field_datetime[!(field_datetime$SITE.CODE=="VAUL" & 
#                                     field_datetime$DATE =="2020-07-06"),]
## 1) ID and store all servicing times in a list
serviceSTARTz = list()
serviceENDz = list()
for(i in sitenamez){
  serviceSTARTz[[i]] = field_datetime$TIME.Last.good.sonde.measurements[field_datetime$SITE.CODE==i & field_datetime$Notes!="setup"& field_datetime$Notes!="reinstall"]
  serviceENDz[[i]] = field_datetime$TIME.First.returned.good.sonde.measurements[field_datetime$SITE.CODE==i & field_datetime$Notes!="setup" & field_datetime$Notes!="reinstall"]
}
#
servicez = list()
for(i in sitenamez){
  for(z in 1:length(serviceSTARTz[[i]])){
    servicez[[i]][[z]] = seq.POSIXt(from = serviceSTARTz[[i]][z], to = serviceENDz[[i]][z], by = 1)
  }
  servicez[[i]] = do.call("c",servicez[[i]])
}
## 2) replace data from servicing times with NA and ID these rows with "servicing" in notes column
### Note that this code replaces all nitrate- and abs-related data with NAs but maintains************TKH working here***************
# for(i in sitenamez){
#  sitedataz[[i]][sitedataz[[i]]$datetimeAK %in% servicez[[i]],][c(4:270, 277:286)] = NA
#  sitedataz[[i]][sitedataz[[i]]$datetimeAK %in% servicez[[i]],]$notes = "servicing"
#}

#### Compile burst measurements for plotting #####
# Compiled burst data is for plotting only at this stage! Uncompiled data should be saved and cleaned with all burst measurements intact so that bursts can be cleaned later

sitedataz.st = sitedataz

for( i in sitenamez){
  sitedataz.st[[i]]$datetimeAK = lubridate::round_date(as.POSIXct(format(as.POSIXct(sitedataz.st[[i]]$datetimeAK,format="%Y-%m-%d %H:%M:%S"),format="%Y-%m-%d %H:%M"), "%Y-%m-%d %H:%M", tz="America/Anchorage"), "15 minutes")
  sitedataz.st[[i]] = sitedataz.st[[i]] %>%
    select(datetimeAK, nitrateuM, abs254, abs350, specave, darkvaluefit, fitRMSE, int_TC, lamptimecum, relhum, mainV) %>%
    group_by(datetimeAK) %>%
    summarize_all(list(~mean(.),~sd(.)), na.rm = TRUE)
  names(sitedataz.st[[i]]) = gsub("_mean", ".mn", names(sitedataz.st[[i]]))
  names(sitedataz.st[[i]]) = gsub("_sd", ".SD", names(sitedataz.st[[i]]))
}


#### Plot burst-compiled data and save plots ####

makePlotSUNA = function(data){
  data = data[!is.na(data$nitrateuM.mn),]
  par(mfrow=c(3,3), mar=c(7,4,2,1.5))
  # lamp time #
  plot(as.POSIXct(data$datetimeAK, tz="America/Anchorage"),(data$lamptimecum.mn),pch=20,col="black", main="cumulative lamp time (must be < red line)", xlab="date/time", ylab="sec", ylim=c(0, 3.3e+6))
  abline(h=3.24e+6, col="red")
  # internal humidity #
  plot(as.POSIXct(data$datetimeAK, tz="America/Anchorage"),(data$relhum.mn),pch=20,col="black", main="internal humidity (30% = Warning, 50% = BAD!)", xlab="date/time", ylab="%", ylim=c(0,50))
  abline(h=30, col="yellow"); abline(h=50, col="red")
  # battery #
  plot(as.POSIXct(data$datetimeAK, tz="America/Anchorage"), (data$mainV.mn),pch=20,col="black", main="battery", xlab="date/time", ylab="volts")
  # internal temp #
  plot(as.POSIXct(data$datetimeAK, tz="America/Anchorage"),(data$int_TC.mn),pch=20,col="grey", main="internal temp.", ylab="temp. (C)", xlab="date/time")
  # RMSE #
  plot(as.POSIXct(data$datetimeAK, tz="America/Anchorage"), (data$darkvaluefit.mn),pch=20,col="black", main="Darkframe value used for fit (500-1000 is ideal)", xlab="date/time", ylab="counts")
  abline(h=c(500,1000), col="red")
  # RMSE #
  plot(as.POSIXct(data$datetimeAK, tz="America/Anchorage"), (data$fitRMSE.mn),pch=20,col="black", main="RMSE (< 0.001 is ideal)", xlab="date/time", ylab="RMSE")
  abline(h=0.001, col="red")
  # abs 254 #
  plot(as.POSIXct(data$datetimeAK, tz="America/Anchorage"),(data$abs254.mn),pch=20,col="brown", main="ABS@254nm (<1.3 is ideal)", ylab="abs", xlab="date/time")
  arrows(as.POSIXct(data$datetimeAK, tz="America/Anchorage"), data$abs254.mn-data$abs254.SD, 
         as.POSIXct(data$datetimeAK, tz="America/Anchorage"), data$abs254.mn+data$abs254.SD,
         length=.01, angle=90, code=3, col="grey", lwd=.5)
  abline(h=1.3, col="red")
  # abs 350 #
  plot(as.POSIXct(data$datetimeAK, tz="America/Anchorage"),(data$abs350.mn),pch=20,col="brown", main="ABS@350nm (<1.3 is ideal)", ylab="abs", xlab="date/time")
  arrows(as.POSIXct(data$datetimeAK, tz="America/Anchorage"), data$abs350.mn-data$abs350.SD, 
         as.POSIXct(data$datetimeAK, tz="America/Anchorage"), data$abs350.mn+data$abs350.SD,
         length=.01, angle=90, code=3, col="grey", lwd=.5)
  abline(h=1.3, col="red")
  # nitrate #
  plot(as.POSIXct(data$datetimeAK, tz="America/Anchorage"),(data$nitrateuM.mn),pch=20,col="purple", main="Nitrate", ylab="uM", xlab="date/time")
  arrows(as.POSIXct(data$datetimeAK, tz="America/Anchorage"), data$nitrateuM.mn-data$nitrateuM.SD, 
         as.POSIXct(data$datetimeAK, tz="America/Anchorage"), data$nitrateuM.mn+data$nitrateuM.SD,
         length=.01, angle=90, code=3, col="grey", lwd=.5)
  par(mfrow=c(1,1), mar=c(5.1, 4.1, 4.1, 2.1))
}

for( i in sitenamez){
  pdf(paste("SUNA_data/from_internal_harddrive/plots/", i, ".SUNA.st.pdf", sep=""), width = 14, height =9, onefile=FALSE)
  makePlotSUNA(sitedataz.st[[i]])
  dev.off()
}

#### Save burst un-compiled data ####

SUNA.processed = bind_rows(sitedataz, .id = "site.ID")
write.csv(SUNA.processed, "SUNA_data/from_internal_harddrive/processed/SUNA.processed.csv")
